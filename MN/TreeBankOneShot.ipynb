{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# Created by: BoyuanJiang\n",
    "# College of Information Science & Electronic Engineering,ZheJiang University\n",
    "# Email: ginger188@gmail.com\n",
    "# Copyright (c) 2017\n",
    "\n",
    "# @Time    :17-8-29 22:26\n",
    "# @FILE    :mainOmniglot.py\n",
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "from data_loader import TreeBankDataset\n",
    "from OmniglotBuilder import OmniglotBuilder\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup\n",
    "batch_size = 2\n",
    "# what is fce/full context embedding?\n",
    "# fce = True\n",
    "classes_per_set = 5\n",
    "samples_per_class = 1\n",
    "channels = 1\n",
    "# Training setup\n",
    "total_epochs = 100\n",
    "total_train_batches = 1000\n",
    "total_val_batches = 250\n",
    "total_test_batches = 500\n",
    "best_val_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TreeBankDataset(batch_size=batch_size, classes_per_set=classes_per_set,\n",
    "                            samples_per_class=samples_per_class, seed=2017, shuffle=True, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLayer(in_channels, out_channels, keep_prob=0.0):\n",
    "    \"\"\"3*3 convolution with padding,ever time call it the output size become half\"\"\"\n",
    "    cnn_seq = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "        nn.ReLU(True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        nn.Dropout(keep_prob)\n",
    "    )\n",
    "    return cnn_seq\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, vector_dim, output_size, use_cuda, batch_size=1):\n",
    "        super(Classifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Initial a muti-layer Bidirectional LSTM\n",
    "        :param layer_size: a list of each layer'size\n",
    "        :param batch_size: \n",
    "        :param vector_dim: \n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vector_dim = vector_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.use_cuda = use_cuda\n",
    "        self.lstm = nn.LSTM(input_size=self.vector_dim, num_layers=self.num_layers, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, output_size)\n",
    "        self.hidden = self.init_hidden(self.use_cuda)\n",
    "\n",
    "    def init_hidden(self,use_cuda):\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(self.lstm.num_layers, self.batch_size, self.lstm.hidden_size)).cuda(),\n",
    "                    Variable(torch.zeros(self.lstm.num_layers, self.batch_size, self.lstm.hidden_size)).cuda())\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.lstm.num_layers, self.batch_size, self.lstm.hidden_size)),\n",
    "                    Variable(torch.zeros(self.lstm.num_layers, self.batch_size, self.lstm.hidden_size)))\n",
    "\n",
    "    def repackage_hidden(self,h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == Variable:\n",
    "            return Variable(h.data)\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.hidden = self.repackage_hidden(self.hidden)\n",
    "        #lstm input is in the shape of (sequence len, batch_size, input_dim)\n",
    "#         for i in range(inputs.size()[1]):\n",
    "#             #should stop when found padding char...\n",
    "#             output, self.hidden = self.lstm(inputs[:,i:i+1,:], self.hidden)\n",
    "#         #return the last hidden state\n",
    "#         last_hidden = output[:,0,:]\n",
    "        output, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        last_hidden = output[:,-1,:]\n",
    "        return self.linear(last_hidden)\n",
    "\n",
    "class AttentionalClassify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionalClassify, self).__init__()\n",
    "\n",
    "    def forward(self, similarities, support_set_y):\n",
    "        \"\"\"\n",
    "        Products pdfs over the support set classes for the target set image.\n",
    "        :param similarities: A tensor with cosine similarites of size[batch_size,sequence_length]\n",
    "        :param support_set_y:[batch_size,sequence_length,classes_num]\n",
    "        :return: Softmax pdf shape[batch_size,classes_num]\n",
    "        \"\"\"\n",
    "        softmax = nn.Softmax()\n",
    "        softmax_similarities = softmax(similarities)\n",
    "        preds = softmax_similarities.unsqueeze(1).bmm(support_set_y).squeeze()\n",
    "        return preds\n",
    "\n",
    "class DistanceNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This model calculates the cosine distance between each of the support set embeddings and the target image embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistanceNetwork, self).__init__()\n",
    "\n",
    "    def forward(self, support_set, input_image):\n",
    "        \"\"\"\n",
    "        forward implement\n",
    "        :param support_set:the embeddings of the support set images.shape[sequence_length,batch_size,64]\n",
    "        :param input_image: the embedding of the target image,shape[batch_size,64]\n",
    "        :return:shape[batch_size,sequence_length]\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        similarities = []\n",
    "        for support_image in support_set:\n",
    "            sum_support = torch.sum(torch.pow(support_image, 2), 1)\n",
    "            support_manitude = sum_support.clamp(eps, float(\"inf\")).rsqrt()\n",
    "            dot_product = input_image.unsqueeze(1).bmm(support_image.unsqueeze(2)).squeeze()\n",
    "            cosine_similarity = dot_product * support_manitude\n",
    "            similarities.append(cosine_similarity)\n",
    "        similarities = torch.stack(similarities)\n",
    "        return similarities.t()\n",
    "\n",
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, batch_size=32, num_lstm_hidden=100, sequence_embedding_size=100, learning_rate=1e-3, num_classes_per_set=5, \\\n",
    "                 num_samples_per_class=1, input_embedding_dim=300, use_cuda=True):\n",
    "        \"\"\"\n",
    "        This is our main network\n",
    "        :param batch_size:\n",
    "        :param num_channels:\n",
    "        :param learning_rate:\n",
    "        :param fce: Flag indicating whether to use full context embeddings(i.e. apply an LSTM on the CNN embeddings)\n",
    "        :param num_classes_per_set:\n",
    "        :param num_samples_per_class:\n",
    "        :param image_size:\n",
    "        \"\"\"\n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes_per_set = num_classes_per_set\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        #todo: customize number of layers\n",
    "        self.g = Classifier(hidden_size=num_lstm_hidden, num_layers=1, vector_dim=input_embedding_dim, output_size=sequence_embedding_size, use_cuda=use_cuda)\n",
    "        self.dn = DistanceNetwork()\n",
    "        self.classify = AttentionalClassify()\n",
    "\n",
    "    def forward(self, support_set_images, support_set_y_one_hot, target_image, target_y):\n",
    "        \"\"\"\n",
    "        Main process of the network\n",
    "        :param support_set_images: shape[batch_size,sequence_length,num_channels,image_size,image_size]\n",
    "        :param support_set_y_one_hot: shape[batch_size,sequence_length,num_classes_per_set]\n",
    "        :param target_image: shape[batch_size,num_channels,image_size,image_size]\n",
    "        :param target_y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # produce embeddings for support set images\n",
    "        encoded_images = []\n",
    "        for i in np.arange(support_set_images.size(1)):\n",
    "            gen_encode = self.g(support_set_images[:, i, :])\n",
    "            encoded_images.append(gen_encode)\n",
    "\n",
    "        # produce embeddings for target images\n",
    "        gen_encode = self.g(target_image)\n",
    "        encoded_images.append(gen_encode)\n",
    "        output = torch.stack(encoded_images)\n",
    "\n",
    "        # get similarities between support set embeddings and target\n",
    "        similarities = self.dn(support_set=output[:-1], input_image=output[-1])\n",
    "\n",
    "        # produce predictions for target probabilities\n",
    "        preds = self.classify(similarities, support_set_y=support_set_y_one_hot)\n",
    "\n",
    "        # calculate the accuracy\n",
    "        values, indices = preds.max(1)\n",
    "        accuracy = torch.mean((indices.squeeze() == target_y).float())\n",
    "        crossentropy_loss = F.cross_entropy(preds, target_y.long())\n",
    "\n",
    "        return accuracy, crossentropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MatchingNetwork(batch_size=1, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO !!! PROPER embedding\n",
    "embed = nn.Embedding(len(data.word_to_idx), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, classes):\n",
    "    b = np.zeros([*y.shape,classes])\n",
    "    for i in range(len(b)):\n",
    "        b[i, np.arange(y[i].shape[0]), y[i]] = 1\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_x, support_y, target_x, target_y =  data.get_train_batch()\n",
    "\n",
    "shape = support_x.shape\n",
    "\n",
    "embed_support_x = embed(Variable(torch.from_numpy(support_x.reshape(shape[0]*shape[1],-1)).long()))\n",
    "\n",
    "embed_support_x = embed_support_x.resize(shape[0],shape[1],shape[2],300)\n",
    "\n",
    "embed_target_x = embed(Variable(torch.from_numpy(target_x).long()))\n",
    "\n",
    "target_y = Variable(torch.from_numpy(target_y), requires_grad=False).squeeze().long()\n",
    "\n",
    "support_y = Variable(torch.from_numpy(one_hot(support_y, classes_per_set)).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(embed_support_x, support_y, embed_target_x, target_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
