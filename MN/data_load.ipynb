{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load('data/dataset.npy')\n",
    "idx_to_word = {}\n",
    "word_to_idx = {}\n",
    "def _process_vocabs(data): \n",
    "        \"\"\"\n",
    "        process all the words and return the idx of each word in the sentence\n",
    "        \"\"\"\n",
    "        all_words = []\n",
    "        for i in data:\n",
    "            all_words = all_words+i\n",
    "        #list of all words in the dataset    \n",
    "        vocab = sorted(list(set(all_words)))\n",
    "        #vocab = ['<pad>'] + vocab\n",
    "        print(\"Vocab processed, {} words in total\".format(len(vocab)))\n",
    "        \n",
    "        idx_to_word = dict(zip(range(len(vocab)), vocab))\n",
    "        word_to_idx = dict(zip(vocab, range(len(vocab))))\n",
    "        return idx_to_word, word_to_idx, np.array([[word_to_idx[word] for word in sequence] for sequence in data]) # this will count as all the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab processed, 9338 words in total\n"
     ]
    }
   ],
   "source": [
    "idx_to_word, word_to_idx, x = _process_vocabs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(idx_to_word))\n",
    "x = np.reshape(x, newshape=(-1,10))\n",
    "np.random.shuffle(x)\n",
    "x_train, x_val, x_test = x[:900], x[900:1000], x[1000:]\n",
    "classes_per_set = 20 # 20 way\n",
    "samples_per_class = 1 # 1 shot\n",
    "indexes = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "datatset = {\"train\": x_train, \"val\": x_val, \"test\":  x_test}\n",
    "use_cache = True\n",
    "#np.reshape(x, newshape=(-1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "def load_data_cache(data_pack, argument=True):\n",
    "        \"\"\" \n",
    "        load the data with batch, \n",
    "        \"\"\"\n",
    "        cached_dataset = []\n",
    "        classes_idx = np.arange(data_pack.shape[0]) # length of classes\n",
    "        samples_idx = np.arange(data_pack.shape[1]) # 10\n",
    "        for _ in range(1000): # what is this?\n",
    "            support_set_x = []\n",
    "            # 20 * 20 * 1 * maxlen\n",
    "            support_set_y = [] # 20 * 20 * 1\n",
    "            target_x = [] # 20 * 122, \n",
    "            target_y = []# 20 * 1\n",
    "            for i in range(batch_size):  \n",
    "                choose_classes = np.random.choice(classes_idx, size=classes_per_set, replace=False) # choose the 20 classes\n",
    "                choose_label = np.random.choice(classes_per_set, size=1) # choose which to query\n",
    "                choose_samples = np.random.choice(samples_idx, size=samples_per_class + 1, replace=False) #\n",
    "                x_temp = data_pack[choose_classes,:] # select 20 classes as support \n",
    "                x_temp = x_temp[:,choose_samples] # \n",
    "                y_temp = np.arange(classes_per_set)\n",
    "                xx = []\n",
    "                for x in x_temp:\n",
    "                    xx.append(list(x[:-1]))\n",
    "                support_set_x.append(xx)\n",
    "                #support_set_x[i] = x_temp[:,:-1] # the samples exept the last sample\n",
    "                support_set_y.append(list(y_temp)) # transpose\n",
    "                target_x.append(list(x_temp[choose_label][-1])) # the query x with the chosen class\n",
    "                target_y.append(y_temp[choose_label]) \n",
    "            cached_dataset.append([support_set_x, support_set_y, target_x, target_y])\n",
    "        return cached_dataset # return the 1000 training epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_datatset = {\"train\": load_data_cache(x_train),\n",
    "                                    \"val\": load_data_cache(x_val),\n",
    "                                    \"test\": load_data_cache(x_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "support_set_x, support_set_y, target_x, target_y = cached_datatset['train'][0]\n",
    "print(len(support_set_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
