
C:\Users\Alfred\Jupyter\matchingnet - Copy>python mymainfornlp.py
[nltk_data] Downloading package treebank to
[nltk_data]     C:\Users\Alfred\AppData\Roaming\nltk_data...
[nltk_data]   Package treebank is already up-to-date!
3914
12408
1152
Batch  0 : loss= 0.9975112080574036
Batch  1 : loss= 0.9733340740203857
Batch  2 : loss= 0.9407966136932373
Batch  3 : loss= 0.9113222360610962
Batch  4 : loss= 0.8595302700996399
Batch  5 : loss= 0.8216331601142883
Batch  6 : loss= 0.7790263295173645
Batch  7 : loss= 0.7316049337387085
Batch  8 : loss= 0.717470645904541
Batch  9 : loss= 0.691260576248169
Batch  10 : loss= 0.6482397317886353
Batch  11 : loss= 0.6405007839202881
Batch  12 : loss= 0.6181660890579224
Batch  13 : loss= 0.606937825679779
Batch  14 : loss= 0.5866526365280151
Batch  15 : loss= 0.5669333934783936
Batch  16 : loss= 0.5778173804283142
Batch  17 : loss= 0.5502994060516357
Batch  18 : loss= 0.4949662685394287
Batch  19 : loss= 0.46682846546173096
Train: Epoch  0 : loss= 0.7090416014194488
Batch  0 : loss= 1.553877830505371  acc= 0.4000000059604645
Batch  1 : loss= 1.5609169006347656  acc= 0.3400000035762787
Batch  2 : loss= 1.585202693939209  acc= 0.30000001192092896
Batch  3 : loss= 1.5797104835510254  acc= 0.23999999463558197
Train: Epoch  0 : loss= 1.5699269771575928  acc= 0.3200000040233135
Batch  0 : loss= 1.6110522747039795  acc= 0.23999999463558197
Batch  1 : loss= 1.6158158779144287  acc= 0.20000000298023224
Batch  2 : loss= 1.5908869504928589  acc= 0.23999999463558197
Batch  3 : loss= 1.5886245965957642  acc= 0.23999999463558197
Batch  4 : loss= 1.5637271404266357  acc= 0.3799999952316284
Batch  5 : loss= 1.5808082818984985  acc= 0.30000001192092896
Batch  6 : loss= 1.5858278274536133  acc= 0.25999999046325684
Batch  7 : loss= 1.5692429542541504  acc= 0.2800000011920929
Test: Epoch  0 : loss= 1.5882482379674911  acc= 0.26749999821186066
Batch  0 : loss= 0.5632996559143066
Batch  1 : loss= 0.5631858110427856
Batch  2 : loss= 0.5657832622528076
Batch  3 : loss= 0.5556461811065674
Batch  4 : loss= 0.5650638937950134
Batch  5 : loss= 0.5651293992996216
Batch  6 : loss= 0.5546196699142456
Batch  7 : loss= 0.5590749979019165
Batch  8 : loss= 0.573032021522522
Batch  9 : loss= 0.5821200013160706
Batch  10 : loss= 0.5732824206352234
Batch  11 : loss= 0.5606486201286316
Batch  12 : loss= 0.5907130241394043
Batch  13 : loss= 0.5760332345962524
Batch  14 : loss= 0.5506591200828552
Batch  15 : loss= 0.5444795489311218
Batch  16 : loss= 0.5073874592781067
Batch  17 : loss= 0.5010679960250854
Batch  18 : loss= 0.4965950846672058
Batch  19 : loss= 0.48487162590026855
Train: Epoch  1 : loss= 0.5516346514225006
Batch  0 : loss= 1.5527560710906982  acc= 0.30000001192092896
Batch  1 : loss= 1.5707796812057495  acc= 0.3199999928474426
Batch  2 : loss= 1.5347110033035278  acc= 0.36000001430511475
Batch  3 : loss= 1.5625396966934204  acc= 0.3400000035762787
Train: Epoch  1 : loss= 1.555196613073349  acc= 0.33000000566244125
Batch  0 : loss= 1.548830270767212  acc= 0.4000000059604645
Batch  1 : loss= 1.5550918579101562  acc= 0.30000001192092896
Batch  2 : loss= 1.5724389553070068  acc= 0.30000001192092896
Batch  3 : loss= 1.590547800064087  acc= 0.18000000715255737
Batch  4 : loss= 1.5235304832458496  acc= 0.4399999976158142
Batch  5 : loss= 1.5475687980651855  acc= 0.30000001192092896
Batch  6 : loss= 1.5288907289505005  acc= 0.3799999952316284
Batch  7 : loss= 1.561435580253601  acc= 0.25999999046325684
Test: Epoch  1 : loss= 1.5535418093204498  acc= 0.3200000040233135
Batch  0 : loss= 0.5100864171981812
Batch  1 : loss= 0.5157005786895752
Batch  2 : loss= 0.5135040283203125
Batch  3 : loss= 0.5111396908760071
Batch  4 : loss= 0.5348213911056519
Batch  5 : loss= 0.545514702796936
Batch  6 : loss= 0.5478662252426147
Batch  7 : loss= 0.5328491926193237
Batch  8 : loss= 0.5593017339706421
Batch  9 : loss= 0.5473450422286987
Batch  10 : loss= 0.5364789366722107
Batch  11 : loss= 0.5205109119415283
Batch  12 : loss= 0.5220927000045776
Batch  13 : loss= 0.5113700032234192
Batch  14 : loss= 0.5054662227630615
Batch  15 : loss= 0.5147687792778015
Batch  16 : loss= 0.4734783172607422
Batch  17 : loss= 0.4703088402748108
Batch  18 : loss= 0.4680998921394348
Batch  19 : loss= 0.45170921087265015
Train: Epoch  2 : loss= 0.514620640873909
Batch  0 : loss= 1.5382740497589111  acc= 0.30000001192092896
Batch  1 : loss= 1.5369482040405273  acc= 0.4000000059604645
Batch  2 : loss= 1.5043587684631348  acc= 0.4000000059604645
Batch  3 : loss= 1.535900592803955  acc= 0.41999998688697815
Train: Epoch  2 : loss= 1.528870403766632  acc= 0.380000002682209
Batch  0 : loss= 1.5608973503112793  acc= 0.30000001192092896
Batch  1 : loss= 1.5374128818511963  acc= 0.4000000059604645
Batch  2 : loss= 1.5648573637008667  acc= 0.3199999928474426
Batch  3 : loss= 1.5605707168579102  acc= 0.30000001192092896
Batch  4 : loss= 1.5443300008773804  acc= 0.41999998688697815
Batch  5 : loss= 1.5181307792663574  acc= 0.41999998688697815
Batch  6 : loss= 1.5767571926116943  acc= 0.30000001192092896
Batch  7 : loss= 1.5619497299194336  acc= 0.30000001192092896
Test: Epoch  2 : loss= 1.5531132519245148  acc= 0.3450000025331974
Batch  0 : loss= 0.4927542209625244
Batch  1 : loss= 0.5284268856048584
Batch  2 : loss= 0.5183397531509399
Batch  3 : loss= 0.5196669101715088
Batch  4 : loss= 0.5310071110725403
Batch  5 : loss= 0.5399951338768005
Batch  6 : loss= 0.524152934551239
Batch  7 : loss= 0.5132064819335938
Batch  8 : loss= 0.504793643951416
Batch  9 : loss= 0.5062633752822876
Batch  10 : loss= 0.5125336647033691
Batch  11 : loss= 0.5119616985321045
Batch  12 : loss= 0.4916130304336548
Batch  13 : loss= 0.48581600189208984
Batch  14 : loss= 0.4934122562408447
Batch  15 : loss= 0.4793705344200134
Batch  16 : loss= 0.45810067653656006
Batch  17 : loss= 0.487582266330719
Batch  18 : loss= 0.47736746072769165
Batch  19 : loss= 0.47310346364974976
Train: Epoch  3 : loss= 0.5024733752012253
Batch  0 : loss= 1.4856306314468384  acc= 0.5
Batch  1 : loss= 1.537400722503662  acc= 0.30000001192092896
Batch  2 : loss= 1.5446274280548096  acc= 0.2800000011920929
Batch  3 : loss= 1.5121649503707886  acc= 0.41999998688697815
Train: Epoch  3 : loss= 1.5199559330940247  acc= 0.375
Batch  0 : loss= 1.5335681438446045  acc= 0.41999998688697815
Batch  1 : loss= 1.5164357423782349  acc= 0.4399999976158142
Batch  2 : loss= 1.5756933689117432  acc= 0.2800000011920929
Batch  3 : loss= 1.5245695114135742  acc= 0.3799999952316284
Batch  4 : loss= 1.5457792282104492  acc= 0.3400000035762787
Batch  5 : loss= 1.5052366256713867  acc= 0.46000000834465027
Batch  6 : loss= 1.5526498556137085  acc= 0.3199999928474426
Batch  7 : loss= 1.5140539407730103  acc= 0.4399999976158142
Test: Epoch  3 : loss= 1.533498302102089  acc= 0.38499999791383743
Batch  0 : loss= 0.5028374195098877
Batch  1 : loss= 0.5203721523284912
Batch  2 : loss= 0.5108485221862793
Batch  3 : loss= 0.4972856044769287
Batch  4 : loss= 0.49967700242996216
Batch  5 : loss= 0.5003101825714111
Batch  6 : loss= 0.5026204586029053
Batch  7 : loss= 0.5075632929801941
Batch  8 : loss= 0.4916455149650574
Batch  9 : loss= 0.49383676052093506
Batch  10 : loss= 0.49779409170150757
Batch  11 : loss= 0.4899269938468933
Batch  12 : loss= 0.48076772689819336
Batch  13 : loss= 0.5022010803222656
Batch  14 : loss= 0.4945860505104065
Batch  15 : loss= 0.49891573190689087
Batch  16 : loss= 0.4791485667228699
Batch  17 : loss= 0.4902019500732422
Batch  18 : loss= 0.47494804859161377
Batch  19 : loss= 0.4575786590576172
Train: Epoch  4 : loss= 0.4946532905101776
Batch  0 : loss= 1.5170615911483765  acc= 0.41999998688697815
Batch  1 : loss= 1.4567357301712036  acc= 0.6600000262260437
Batch  2 : loss= 1.5608243942260742  acc= 0.36000001430511475
Batch  3 : loss= 1.48223078250885  acc= 0.5
Train: Epoch  4 : loss= 1.504213124513626  acc= 0.48500000685453415
Batch  0 : loss= 1.5649826526641846  acc= 0.2800000011920929
Batch  1 : loss= 1.5430477857589722  acc= 0.3199999928474426
Batch  2 : loss= 1.5071296691894531  acc= 0.47999998927116394
Batch  3 : loss= 1.550125241279602  acc= 0.36000001430511475
Batch  4 : loss= 1.5500118732452393  acc= 0.4000000059604645
Batch  5 : loss= 1.5146898031234741  acc= 0.4000000059604645
Batch  6 : loss= 1.4770569801330566  acc= 0.47999998927116394
Batch  7 : loss= 1.5441230535507202  acc= 0.3400000035762787
Test: Epoch  4 : loss= 1.5313958823680878  acc= 0.3825000002980232
Batch  0 : loss= 0.48494046926498413
Batch  1 : loss= 0.4859354496002197
Batch  2 : loss= 0.4906885623931885
Batch  3 : loss= 0.49731016159057617
Batch  4 : loss= 0.4937613606452942
Batch  5 : loss= 0.49104011058807373
Batch  6 : loss= 0.49498867988586426
Batch  7 : loss= 0.4933162331581116
Batch  8 : loss= 0.48795807361602783
Batch  9 : loss= 0.5111646056175232
Batch  10 : loss= 0.5010420083999634
Batch  11 : loss= 0.5063125491142273
Batch  12 : loss= 0.49539655447006226
Batch  13 : loss= 0.5132563710212708
Batch  14 : loss= 0.4924926161766052
Batch  15 : loss= 0.4815937876701355
Batch  16 : loss= 0.4630849361419678
Batch  17 : loss= 0.4651084542274475
Batch  18 : loss= 0.4588075280189514
Batch  19 : loss= 0.4565975069999695
Train: Epoch  5 : loss= 0.48823980093002317
Batch  0 : loss= 1.4926419258117676  acc= 0.47999998927116394
Batch  1 : loss= 1.5203570127487183  acc= 0.41999998688697815
Batch  2 : loss= 1.477380633354187  acc= 0.41999998688697815
Batch  3 : loss= 1.5014238357543945  acc= 0.3799999952316284
Train: Epoch  5 : loss= 1.4979508519172668  acc= 0.42499998956918716
Batch  0 : loss= 1.519229531288147  acc= 0.4000000059604645
Batch  1 : loss= 1.5046566724777222  acc= 0.47999998927116394
Batch  2 : loss= 1.554498553276062  acc= 0.2800000011920929
Batch  3 : loss= 1.5133476257324219  acc= 0.36000001430511475
Batch  4 : loss= 1.586991786956787  acc= 0.25999999046325684
Batch  5 : loss= 1.5433173179626465  acc= 0.36000001430511475
Batch  6 : loss= 1.4964361190795898  acc= 0.41999998688697815
Batch  7 : loss= 1.5465996265411377  acc= 0.3799999952316284
Test: Epoch  5 : loss= 1.5331346541643143  acc= 0.3674999997019768
Batch  0 : loss= 0.4795560836791992
Batch  1 : loss= 0.4818006753921509
Batch  2 : loss= 0.48407721519470215
Batch  3 : loss= 0.48603856563568115
Batch  4 : loss= 0.4792870283126831
Batch  5 : loss= 0.5004168748855591
Batch  6 : loss= 0.49718648195266724
Batch  7 : loss= 0.503896951675415
Batch  8 : loss= 0.4942106604576111
Batch  9 : loss= 0.51531982421875
Batch  10 : loss= 0.49189913272857666
Batch  11 : loss= 0.48324888944625854
Batch  12 : loss= 0.4744030833244324
Batch  13 : loss= 0.4731627106666565
Batch  14 : loss= 0.4763922095298767
Batch  15 : loss= 0.47455573081970215
Batch  16 : loss= 0.46842825412750244
Batch  17 : loss= 0.45676934719085693
Batch  18 : loss= 0.45176196098327637
Batch  19 : loss= 0.45627278089523315
Train: Epoch  6 : loss= 0.48143422305583955
Batch  0 : loss= 1.4810067415237427  acc= 0.5400000214576721
Batch  1 : loss= 1.489020586013794  acc= 0.5
Batch  2 : loss= 1.4933477640151978  acc= 0.5199999809265137
Batch  3 : loss= 1.4523872137069702  acc= 0.47999998927116394
Train: Epoch  6 : loss= 1.4789405763149261  acc= 0.5099999979138374
Batch  0 : loss= 1.55775785446167  acc= 0.3199999928474426
Batch  1 : loss= 1.6181869506835938  acc= 0.14000000059604645
Batch  2 : loss= 1.5564543008804321  acc= 0.3199999928474426
Batch  3 : loss= 1.5432029962539673  acc= 0.3400000035762787
Batch  4 : loss= 1.5539988279342651  acc= 0.3199999928474426
Batch  5 : loss= 1.6075313091278076  acc= 0.2199999988079071
Batch  6 : loss= 1.4590343236923218  acc= 0.5199999809265137
Batch  7 : loss= 1.5388516187667847  acc= 0.36000001430511475
Test: Epoch  6 : loss= 1.5543772727251053  acc= 0.31749999709427357
Batch  0 : loss= 0.4677867293357849
Batch  1 : loss= 0.4911889433860779
Batch  2 : loss= 0.4889897108078003
Batch  3 : loss= 0.4961037039756775
Batch  4 : loss= 0.48875290155410767
Batch  5 : loss= 0.5087884068489075
Batch  6 : loss= 0.4919746518135071
Batch  7 : loss= 0.47838592529296875
Batch  8 : loss= 0.4773419499397278
Batch  9 : loss= 0.4770621061325073
Batch  10 : loss= 0.4768582582473755
Batch  11 : loss= 0.4754387140274048
Batch  12 : loss= 0.48054903745651245
Batch  13 : loss= 0.47548139095306396
Batch  14 : loss= 0.4645646810531616
Batch  15 : loss= 0.4686916470527649
Batch  16 : loss= 0.44746941328048706
Batch  17 : loss= 0.46390193700790405
Batch  18 : loss= 0.46353113651275635
Batch  19 : loss= 0.4630424976348877
Train: Epoch  7 : loss= 0.47729518711566926
Batch  0 : loss= 1.4941214323043823  acc= 0.47999998927116394
Batch  1 : loss= 1.5298913717269897  acc= 0.3799999952316284
Batch  2 : loss= 1.4664247035980225  acc= 0.46000000834465027
Batch  3 : loss= 1.4946987628936768  acc= 0.4000000059604645
Train: Epoch  7 : loss= 1.4962840676307678  acc= 0.4299999997019768
Batch  0 : loss= 1.4648175239562988  acc= 0.47999998927116394
Batch  1 : loss= 1.5429866313934326  acc= 0.3400000035762787
Batch  2 : loss= 1.5381684303283691  acc= 0.36000001430511475
Batch  3 : loss= 1.5877386331558228  acc= 0.23999999463558197
Batch  4 : loss= 1.5143284797668457  acc= 0.30000001192092896
Batch  5 : loss= 1.5168871879577637  acc= 0.4000000059604645
Batch  6 : loss= 1.5452145338058472  acc= 0.3199999928474426
Batch  7 : loss= 1.5291019678115845  acc= 0.3799999952316284
Test: Epoch  7 : loss= 1.5299054235219955  acc= 0.3525000009685755
Batch  0 : loss= 0.4742344617843628
Batch  1 : loss= 0.48535507917404175
Batch  2 : loss= 0.48429614305496216
Batch  3 : loss= 0.46571922302246094
Batch  4 : loss= 0.4736163020133972
Batch  5 : loss= 0.47035855054855347
Batch  6 : loss= 0.47129273414611816
Batch  7 : loss= 0.46883755922317505
Batch  8 : loss= 0.47105979919433594
Batch  9 : loss= 0.4754212498664856
Batch  10 : loss= 0.4741513729095459
Batch  11 : loss= 0.47327983379364014
Batch  12 : loss= 0.4519270658493042
Batch  13 : loss= 0.4668921232223511
Batch  14 : loss= 0.4812048673629761
Batch  15 : loss= 0.4738558530807495
Batch  16 : loss= 0.45811188220977783
Batch  17 : loss= 0.46784698963165283
Batch  18 : loss= 0.46306461095809937
Batch  19 : loss= 0.43954038619995117
Train: Epoch  8 : loss= 0.46950330436229704
Batch  0 : loss= 1.5134772062301636  acc= 0.36000001430511475
Batch  1 : loss= 1.4532595872879028  acc= 0.47999998927116394
Batch  2 : loss= 1.4523415565490723  acc= 0.5199999809265137
Batch  3 : loss= 1.4318045377731323  acc= 0.5600000023841858
Train: Epoch  8 : loss= 1.4627207219600677  acc= 0.47999999672174454
Batch  0 : loss= 1.5180073976516724  acc= 0.3400000035762787
Batch  1 : loss= 1.5187150239944458  acc= 0.41999998688697815
Batch  2 : loss= 1.5655882358551025  acc= 0.3799999952316284
Batch  3 : loss= 1.5287318229675293  acc= 0.4000000059604645
Batch  4 : loss= 1.5287699699401855  acc= 0.4000000059604645
Batch  5 : loss= 1.5588123798370361  acc= 0.3400000035762787
Batch  6 : loss= 1.5667009353637695  acc= 0.2800000011920929
Batch  7 : loss= 1.5090316534042358  acc= 0.4000000059604645
Test: Epoch  8 : loss= 1.5367946773767471  acc= 0.3700000010430813
Batch  0 : loss= 0.4624045491218567
Batch  1 : loss= 0.4632027745246887
Batch  2 : loss= 0.46651870012283325
Batch  3 : loss= 0.46716219186782837
Batch  4 : loss= 0.47081923484802246
Batch  5 : loss= 0.47103214263916016
Batch  6 : loss= 0.47141164541244507
Batch  7 : loss= 0.47422027587890625
Batch  8 : loss= 0.4602397680282593
Batch  9 : loss= 0.47268056869506836
Batch  10 : loss= 0.48900657892227173
Batch  11 : loss= 0.4780092239379883
Batch  12 : loss= 0.46856868267059326
Batch  13 : loss= 0.48479151725769043
Batch  14 : loss= 0.4678024649620056
Batch  15 : loss= 0.4537060260772705
Batch  16 : loss= 0.44529062509536743
Batch  17 : loss= 0.4394029974937439
Batch  18 : loss= 0.4442935585975647
Batch  19 : loss= 0.43791621923446655
Train: Epoch  9 : loss= 0.46442398726940154
Batch  0 : loss= 1.4686431884765625  acc= 0.47999998927116394
Batch  1 : loss= 1.4223566055297852  acc= 0.6000000238418579
Batch  2 : loss= 1.4876900911331177  acc= 0.5
Batch  3 : loss= 1.4463330507278442  acc= 0.6000000238418579
Train: Epoch  9 : loss= 1.4562557339668274  acc= 0.5450000092387199
Batch  0 : loss= 1.5203982591629028  acc= 0.4399999976158142
Batch  1 : loss= 1.5554431676864624  acc= 0.36000001430511475
Batch  2 : loss= 1.566041111946106  acc= 0.3199999928474426
Batch  3 : loss= 1.5291301012039185  acc= 0.3400000035762787
Batch  4 : loss= 1.507567286491394  acc= 0.4399999976158142
Batch  5 : loss= 1.5154953002929688  acc= 0.3400000035762787
Batch  6 : loss= 1.5750880241394043  acc= 0.3400000035762787
Batch  7 : loss= 1.545533299446106  acc= 0.3199999928474426
Test: Epoch  9 : loss= 1.5393370687961578  acc= 0.36250000074505806
Batch  0 : loss= 0.46292293071746826
Batch  1 : loss= 0.460294246673584
Batch  2 : loss= 0.46123117208480835
Batch  3 : loss= 0.46456921100616455
Batch  4 : loss= 0.45213842391967773
Batch  5 : loss= 0.4755977392196655
Batch  6 : loss= 0.48236703872680664
Batch  7 : loss= 0.47600889205932617
Batch  8 : loss= 0.4686521887779236
Batch  9 : loss= 0.48357850313186646
Batch  10 : loss= 0.47608858346939087
Batch  11 : loss= 0.46021103858947754
Batch  12 : loss= 0.4563997983932495
Batch  13 : loss= 0.45263969898223877
Batch  14 : loss= 0.45313096046447754
Batch  15 : loss= 0.4452701210975647
Batch  16 : loss= 0.4514771103858948
Batch  17 : loss= 0.439450740814209
Batch  18 : loss= 0.4365720748901367
Batch  19 : loss= 0.4394829273223877
Train: Epoch  10 : loss= 0.4599041700363159
Batch  0 : loss= 1.4945625066757202  acc= 0.4000000059604645
Batch  1 : loss= 1.4820283651351929  acc= 0.5
Batch  2 : loss= 1.5031883716583252  acc= 0.4000000059604645
Batch  3 : loss= 1.451781153678894  acc= 0.46000000834465027
Train: Epoch  10 : loss= 1.482890099287033  acc= 0.4400000050663948
Batch  0 : loss= 1.5465229749679565  acc= 0.30000001192092896
Batch  1 : loss= 1.544871211051941  acc= 0.36000001430511475
Batch  2 : loss= 1.5312718152999878  acc= 0.3799999952316284
Batch  3 : loss= 1.5714491605758667  acc= 0.30000001192092896
Batch  4 : loss= 1.533319354057312  acc= 0.3799999952316284
Batch  5 : loss= 1.529868483543396  acc= 0.4000000059604645
Batch  6 : loss= 1.5087361335754395  acc= 0.47999998927116394
Batch  7 : loss= 1.482316255569458  acc= 0.47999998927116394
Test: Epoch  10 : loss= 1.5310444235801697  acc= 0.38500000163912773
Batch  0 : loss= 0.4369359612464905
Batch  1 : loss= 0.4537588953971863
Batch  2 : loss= 0.4774823784828186
Batch  3 : loss= 0.46278679370880127
Batch  4 : loss= 0.4654904007911682
Batch  5 : loss= 0.4654555916786194
Batch  6 : loss= 0.47672998905181885
Batch  7 : loss= 0.456584095954895
Batch  8 : loss= 0.45592761039733887
Batch  9 : loss= 0.45097559690475464
Batch  10 : loss= 0.45312267541885376
Batch  11 : loss= 0.45462822914123535
Batch  12 : loss= 0.45439428091049194
Batch  13 : loss= 0.44522690773010254
Batch  14 : loss= 0.44927793741226196
Batch  15 : loss= 0.447102427482605
Batch  16 : loss= 0.4297063946723938
Batch  17 : loss= 0.4301474690437317
Batch  18 : loss= 0.45124977827072144
Batch  19 : loss= 0.4425853490829468
Train: Epoch  11 : loss= 0.45297843813896177
Batch  0 : loss= 1.4606820344924927  acc= 0.5
Batch  1 : loss= 1.498756766319275  acc= 0.47999998927116394
Batch  2 : loss= 1.4544950723648071  acc= 0.5
Batch  3 : loss= 1.446132779121399  acc= 0.5
Train: Epoch  11 : loss= 1.4650166630744934  acc= 0.494999997317791
Batch  0 : loss= 1.5705440044403076  acc= 0.2800000011920929
Batch  1 : loss= 1.564864158630371  acc= 0.30000001192092896
Batch  2 : loss= 1.5636656284332275  acc= 0.3199999928474426
Batch  3 : loss= 1.5179530382156372  acc= 0.46000000834465027
Batch  4 : loss= 1.5646880865097046  acc= 0.3400000035762787
Batch  5 : loss= 1.5637074708938599  acc= 0.25999999046325684
Batch  6 : loss= 1.5707106590270996  acc= 0.23999999463558197
Batch  7 : loss= 1.5319546461105347  acc= 0.36000001430511475
Test: Epoch  11 : loss= 1.5560109615325928  acc= 0.3200000021606684
Batch  0 : loss= 0.45574527978897095
Batch  1 : loss= 0.4560418725013733
Batch  2 : loss= 0.46786433458328247
Batch  3 : loss= 0.44981199502944946
Batch  4 : loss= 0.4434194564819336
Batch  5 : loss= 0.447656512260437
Batch  6 : loss= 0.4539274573326111
Batch  7 : loss= 0.45252400636672974
Batch  8 : loss= 0.4544018507003784
Batch  9 : loss= 0.44540637731552124
Batch  10 : loss= 0.4528645873069763
Batch  11 : loss= 0.45399922132492065
Batch  12 : loss= 0.43505364656448364
Batch  13 : loss= 0.43675655126571655
Batch  14 : loss= 0.4630863070487976
Batch  15 : loss= 0.449184775352478
Batch  16 : loss= 0.44804835319519043
Batch  17 : loss= 0.43555402755737305
Batch  18 : loss= 0.44556355476379395
Batch  19 : loss= 0.42839503288269043
Train: Epoch  12 : loss= 0.4487652599811554
Batch  0 : loss= 1.453109622001648  acc= 0.46000000834465027
Batch  1 : loss= 1.4648840427398682  acc= 0.5199999809265137
Batch  2 : loss= 1.4975587129592896  acc= 0.4000000059604645
Batch  3 : loss= 1.4737451076507568  acc= 0.5600000023841858
Train: Epoch  12 : loss= 1.4723243713378906  acc= 0.48499999940395355
Batch  0 : loss= 1.4815183877944946  acc= 0.41999998688697815
Batch  1 : loss= 1.5309269428253174  acc= 0.3799999952316284
Batch  2 : loss= 1.5232009887695312  acc= 0.3799999952316284
Batch  3 : loss= 1.556484580039978  acc= 0.3400000035762787
Batch  4 : loss= 1.5374935865402222  acc= 0.3199999928474426
Batch  5 : loss= 1.4944794178009033  acc= 0.47999998927116394
Batch  6 : loss= 1.4640846252441406  acc= 0.5199999809265137
Batch  7 : loss= 1.508952021598816  acc= 0.4399999976158142
Test: Epoch  12 : loss= 1.5121425688266754  acc= 0.409999992698431
Batch  0 : loss= 0.4306398630142212
Batch  1 : loss= 0.44144368171691895
Batch  2 : loss= 0.44393765926361084
Batch  3 : loss= 0.45057016611099243
Batch  4 : loss= 0.4472907781600952
Batch  5 : loss= 0.444053590297699
Batch  6 : loss= 0.44701552391052246
Batch  7 : loss= 0.44974952936172485
Batch  8 : loss= 0.4436667561531067
Batch  9 : loss= 0.4434424042701721
Batch  10 : loss= 0.4656289219856262
Batch  11 : loss= 0.4546719789505005
Batch  12 : loss= 0.4537307620048523
Batch  13 : loss= 0.44720035791397095
Batch  14 : loss= 0.4642223119735718
Batch  15 : loss= 0.43216127157211304
Batch  16 : loss= 0.4193836450576782
Batch  17 : loss= 0.4250096082687378
Batch  18 : loss= 0.4246472120285034
Batch  19 : loss= 0.42583364248275757
Train: Epoch  13 : loss= 0.4427149832248688
Batch  0 : loss= 1.441726565361023  acc= 0.46000000834465027
Batch  1 : loss= 1.4323456287384033  acc= 0.5799999833106995
Batch  2 : loss= 1.49932861328125  acc= 0.41999998688697815
Batch  3 : loss= 1.475602149963379  acc= 0.5
Train: Epoch  13 : loss= 1.4622507393360138  acc= 0.48999999463558197
Batch  0 : loss= 1.5031105279922485  acc= 0.46000000834465027
Batch  1 : loss= 1.5284597873687744  acc= 0.3400000035762787
Batch  2 : loss= 1.5771337747573853  acc= 0.2800000011920929
Batch  3 : loss= 1.5087308883666992  acc= 0.36000001430511475
Batch  4 : loss= 1.525841236114502  acc= 0.2800000011920929
Batch  5 : loss= 1.5790395736694336  acc= 0.25999999046325684
Batch  6 : loss= 1.522222876548767  acc= 0.3799999952316284
Batch  7 : loss= 1.5404835939407349  acc= 0.36000001430511475
Test: Epoch  13 : loss= 1.5356277823448181  acc= 0.3400000035762787
Batch  0 : loss= 0.4381006956100464
Batch  1 : loss= 0.441053569316864
Batch  2 : loss= 0.44102251529693604
Batch  3 : loss= 0.4464057683944702
Batch  4 : loss= 0.44257277250289917
Batch  5 : loss= 0.43722623586654663
Batch  6 : loss= 0.4655872583389282
Batch  7 : loss= 0.4528235197067261
Batch  8 : loss= 0.45987576246261597
Batch  9 : loss= 0.4492282271385193
Batch  10 : loss= 0.468239426612854
Batch  11 : loss= 0.44619011878967285
Batch  12 : loss= 0.4236330986022949
Batch  13 : loss= 0.43872129917144775
Batch  14 : loss= 0.436165452003479
Batch  15 : loss= 0.4367902874946594
Batch  16 : loss= 0.4252234697341919
Batch  17 : loss= 0.4303995370864868
Batch  18 : loss= 0.4213888645172119
Batch  19 : loss= 0.4210203289985657
Train: Epoch  14 : loss= 0.44108341038227084
Batch  0 : loss= 1.432178020477295  acc= 0.5799999833106995
Batch  1 : loss= 1.4577293395996094  acc= 0.46000000834465027
Batch  2 : loss= 1.4888272285461426  acc= 0.5
Batch  3 : loss= 1.4394794702529907  acc= 0.5600000023841858
Train: Epoch  14 : loss= 1.4545535147190094  acc= 0.5249999985098839
Batch  0 : loss= 1.4926828145980835  acc= 0.4399999976158142
Batch  1 : loss= 1.5131486654281616  acc= 0.3799999952316284
Batch  2 : loss= 1.5444010496139526  acc= 0.2800000011920929
Batch  3 : loss= 1.4987001419067383  acc= 0.4399999976158142
Batch  4 : loss= 1.5343493223190308  acc= 0.36000001430511475
Batch  5 : loss= 1.538035273551941  acc= 0.3199999928474426
Batch  6 : loss= 1.5104118585586548  acc= 0.3799999952316284
Batch  7 : loss= 1.5110044479370117  acc= 0.46000000834465027
Test: Epoch  14 : loss= 1.5178416967391968  acc= 0.3825000002980232
Batch  0 : loss= 0.44004714488983154
Batch  1 : loss= 0.4260699152946472
Batch  2 : loss= 0.45238637924194336
Batch  3 : loss= 0.45016664266586304
Batch  4 : loss= 0.4529321789741516
Batch  5 : loss= 0.44500768184661865
Batch  6 : loss= 0.4591217041015625
Batch  7 : loss= 0.44516634941101074
Batch  8 : loss= 0.4293823838233948
Batch  9 : loss= 0.43699324131011963
Batch  10 : loss= 0.43368661403656006
Batch  11 : loss= 0.4431827664375305
Batch  12 : loss= 0.4295915365219116
Batch  13 : loss= 0.43366163969039917
Batch  14 : loss= 0.43190836906433105
Batch  15 : loss= 0.42891424894332886
Batch  16 : loss= 0.4301629662513733
Batch  17 : loss= 0.4092975854873657
Batch  18 : loss= 0.42829370498657227
Batch  19 : loss= 0.43452388048171997
Train: Epoch  15 : loss= 0.4370248466730118
Batch  0 : loss= 1.4108351469039917  acc= 0.6399999856948853
Batch  1 : loss= 1.4320862293243408  acc= 0.5400000214576721
Batch  2 : loss= 1.4243216514587402  acc= 0.5
Batch  3 : loss= 1.423921823501587  acc= 0.6000000238418579
Train: Epoch  15 : loss= 1.422791212797165  acc= 0.5700000077486038
Batch  0 : loss= 1.51749849319458  acc= 0.3799999952316284
Batch  1 : loss= 1.5399285554885864  acc= 0.36000001430511475
Batch  2 : loss= 1.4907647371292114  acc= 0.41999998688697815
Batch  3 : loss= 1.521376609802246  acc= 0.41999998688697815
Batch  4 : loss= 1.520713448524475  acc= 0.36000001430511475
Batch  5 : loss= 1.5282396078109741  acc= 0.4000000059604645
Batch  6 : loss= 1.5307111740112305  acc= 0.3199999928474426
Batch  7 : loss= 1.5497145652770996  acc= 0.2800000011920929
Test: Epoch  15 : loss= 1.5248683989048004  acc= 0.3674999997019768
Batch  0 : loss= 0.4466801881790161
Batch  1 : loss= 0.44576871395111084
Batch  2 : loss= 0.4505653977394104
Batch  3 : loss= 0.4470255374908447
Batch  4 : loss= 0.4267204999923706
Batch  5 : loss= 0.4417268633842468
Batch  6 : loss= 0.43014979362487793
Batch  7 : loss= 0.4414498805999756
Batch  8 : loss= 0.4362620711326599
Batch  9 : loss= 0.4306323528289795
Batch  10 : loss= 0.44384801387786865
Batch  11 : loss= 0.4392012357711792
Batch  12 : loss= 0.4393705725669861
Batch  13 : loss= 0.41653549671173096
Batch  14 : loss= 0.43211978673934937
Batch  15 : loss= 0.4503759741783142
Batch  16 : loss= 0.43341314792633057
Batch  17 : loss= 0.42847567796707153
Batch  18 : loss= 0.43495315313339233
Batch  19 : loss= 0.4252733588218689
Train: Epoch  16 : loss= 0.4370273858308792
Batch  0 : loss= 1.4958032369613647  acc= 0.47999998927116394
Batch  1 : loss= 1.4232738018035889  acc= 0.5199999809265137
Batch  2 : loss= 1.4660289287567139  acc= 0.5400000214576721
Batch  3 : loss= 1.4070839881896973  acc= 0.5799999833106995
Train: Epoch  16 : loss= 1.4480474889278412  acc= 0.5299999937415123
Batch  0 : loss= 1.5128096342086792  acc= 0.46000000834465027
Batch  1 : loss= 1.5105431079864502  acc= 0.4000000059604645
Batch  2 : loss= 1.4819769859313965  acc= 0.5199999809265137
Batch  3 : loss= 1.596683144569397  acc= 0.25999999046325684
Batch  4 : loss= 1.4613908529281616  acc= 0.46000000834465027
Batch  5 : loss= 1.5606045722961426  acc= 0.3199999928474426
Batch  6 : loss= 1.5177079439163208  acc= 0.3400000035762787
Batch  7 : loss= 1.5343141555786133  acc= 0.3400000035762787
Test: Epoch  16 : loss= 1.5220037996768951  acc= 0.38749999925494194
Batch  0 : loss= 0.41700756549835205
Batch  1 : loss= 0.4304884076118469
Batch  2 : loss= 0.42323702573776245
Batch  3 : loss= 0.43603140115737915
Batch  4 : loss= 0.4290311932563782
Batch  5 : loss= 0.43139535188674927
Batch  6 : loss= 0.43394917249679565
Batch  7 : loss= 0.4376225471496582
Batch  8 : loss= 0.43815428018569946
Batch  9 : loss= 0.42442572116851807
Batch  10 : loss= 0.4285922646522522
Batch  11 : loss= 0.4544387459754944
Batch  12 : loss= 0.4359304904937744
Batch  13 : loss= 0.43238818645477295
Batch  14 : loss= 0.4457125663757324
Batch  15 : loss= 0.4356423020362854
Batch  16 : loss= 0.4063487648963928
Batch  17 : loss= 0.41606801748275757
Batch  18 : loss= 0.4053906202316284
Batch  19 : loss= 0.4188297390937805
Train: Epoch  17 : loss= 0.4290342181921005
Batch  0 : loss= 1.431993842124939  acc= 0.5799999833106995
Batch  1 : loss= 1.4569311141967773  acc= 0.47999998927116394
Batch  2 : loss= 1.4383540153503418  acc= 0.5400000214576721
Batch  3 : loss= 1.4556599855422974  acc= 0.5199999809265137
Train: Epoch  17 : loss= 1.4457347393035889  acc= 0.5299999937415123
Batch  0 : loss= 1.5121182203292847  acc= 0.3400000035762787
Batch  1 : loss= 1.5239341259002686  acc= 0.4000000059604645
Batch  2 : loss= 1.5037575960159302  acc= 0.41999998688697815
Batch  3 : loss= 1.5565497875213623  acc= 0.30000001192092896
Batch  4 : loss= 1.527817964553833  acc= 0.3199999928474426
Batch  5 : loss= 1.5067652463912964  acc= 0.46000000834465027
Batch  6 : loss= 1.485952615737915  acc= 0.5199999809265137
Batch  7 : loss= 1.5786646604537964  acc= 0.2199999988079071
Test: Epoch  17 : loss= 1.5244450271129608  acc= 0.3724999986588955
Batch  0 : loss= 0.4212544560432434
Batch  1 : loss= 0.42894047498703003
Batch  2 : loss= 0.4243599772453308
Batch  3 : loss= 0.4314696788787842
Batch  4 : loss= 0.4350287914276123
Batch  5 : loss= 0.4215816259384155
Batch  6 : loss= 0.4315105080604553
Batch  7 : loss= 0.45018941164016724
Batch  8 : loss= 0.43976891040802
Batch  9 : loss= 0.4324226379394531
Batch  10 : loss= 0.44950681924819946
Batch  11 : loss= 0.4419465661048889
Batch  12 : loss= 0.4195917248725891
Batch  13 : loss= 0.4196023941040039
Batch  14 : loss= 0.4157646894454956
Batch  15 : loss= 0.42883557081222534
Batch  16 : loss= 0.4087029695510864
Batch  17 : loss= 0.42052632570266724
Batch  18 : loss= 0.4047290086746216
Batch  19 : loss= 0.41373807191848755
Train: Epoch  18 : loss= 0.4269735306501389
Batch  0 : loss= 1.4564921855926514  acc= 0.5199999809265137
Batch  1 : loss= 1.402727484703064  acc= 0.5400000214576721
Batch  2 : loss= 1.4681767225265503  acc= 0.5
Batch  3 : loss= 1.4298524856567383  acc= 0.6200000047683716
Train: Epoch  18 : loss= 1.439312219619751  acc= 0.5450000017881393
Batch  0 : loss= 1.5027825832366943  acc= 0.4399999976158142
Batch  1 : loss= 1.5313209295272827  acc= 0.3799999952316284
Batch  2 : loss= 1.5016365051269531  acc= 0.46000000834465027
Batch  3 : loss= 1.513916015625  acc= 0.36000001430511475
Batch  4 : loss= 1.4990288019180298  acc= 0.47999998927116394
Batch  5 : loss= 1.5267689228057861  acc= 0.3400000035762787
Batch  6 : loss= 1.502679467201233  acc= 0.47999998927116394
Batch  7 : loss= 1.5776002407073975  acc= 0.3199999928474426
Test: Epoch  18 : loss= 1.519466683268547  acc= 0.4074999988079071
Batch  0 : loss= 0.42694932222366333
Batch  1 : loss= 0.4176021218299866
Batch  2 : loss= 0.42147642374038696
Batch  3 : loss= 0.4496524930000305
Batch  4 : loss= 0.43301844596862793
Batch  5 : loss= 0.43735235929489136
Batch  6 : loss= 0.4391290545463562
Batch  7 : loss= 0.4438632130622864
Batch  8 : loss= 0.423733651638031
Batch  9 : loss= 0.42458248138427734
Batch  10 : loss= 0.41748958826065063
Batch  11 : loss= 0.42446261644363403
Batch  12 : loss= 0.42400914430618286
Batch  13 : loss= 0.42946404218673706
Batch  14 : loss= 0.4195988178253174
Batch  15 : loss= 0.41614454984664917
Batch  16 : loss= 0.41965222358703613
Batch  17 : loss= 0.40552818775177
Batch  18 : loss= 0.4027416706085205
Batch  19 : loss= 0.42562371492385864
Train: Epoch  19 : loss= 0.4251037061214447
Batch  0 : loss= 1.449323296546936  acc= 0.4399999976158142
Batch  1 : loss= 1.4391734600067139  acc= 0.5400000214576721
Batch  2 : loss= 1.5002614259719849  acc= 0.3799999952316284
Batch  3 : loss= 1.4167351722717285  acc= 0.6000000238418579
Train: Epoch  19 : loss= 1.4513733386993408  acc= 0.49000000953674316
Batch  0 : loss= 1.5051684379577637  acc= 0.3799999952316284
Batch  1 : loss= 1.4537690877914429  acc= 0.5
Batch  2 : loss= 1.4848757982254028  acc= 0.47999998927116394
Batch  3 : loss= 1.4990102052688599  acc= 0.41999998688697815
Batch  4 : loss= 1.5337127447128296  acc= 0.3400000035762787
Batch  5 : loss= 1.547249436378479  acc= 0.25999999046325684
Batch  6 : loss= 1.5307517051696777  acc= 0.3400000035762787
Batch  7 : loss= 1.533239483833313  acc= 0.3400000035762787
Test: Epoch  19 : loss= 1.510972112417221  acc= 0.3824999965727329
Batch  0 : loss= 0.4348548650741577
Batch  1 : loss= 0.4352930188179016
Batch  2 : loss= 0.4287775158882141
Batch  3 : loss= 0.4411013126373291
Batch  4 : loss= 0.42305904626846313
Batch  5 : loss= 0.41757726669311523
Batch  6 : loss= 0.416986882686615
Batch  7 : loss= 0.4230595827102661
Batch  8 : loss= 0.4259801506996155
Batch  9 : loss= 0.4308544993400574
Batch  10 : loss= 0.4204642176628113
Batch  11 : loss= 0.42025279998779297
Batch  12 : loss= 0.4203847646713257
Batch  13 : loss= 0.4150388836860657
Batch  14 : loss= 0.4132257103919983
Batch  15 : loss= 0.4398998022079468
Batch  16 : loss= 0.4210805296897888
Batch  17 : loss= 0.4256250858306885
Batch  18 : loss= 0.41221946477890015
Batch  19 : loss= 0.4215261936187744
Train: Epoch  20 : loss= 0.42436307966709136
Batch  0 : loss= 1.370545506477356  acc= 0.6600000262260437
Batch  1 : loss= 1.4633630514144897  acc= 0.5199999809265137
Batch  2 : loss= 1.3994629383087158  acc= 0.6000000238418579
Batch  3 : loss= 1.4384030103683472  acc= 0.5600000023841858
Train: Epoch  20 : loss= 1.4179436266422272  acc= 0.5850000083446503
Batch  0 : loss= 1.5692821741104126  acc= 0.3400000035762787
Batch  1 : loss= 1.5788054466247559  acc= 0.25999999046325684
Batch  2 : loss= 1.5456902980804443  acc= 0.3400000035762787
Batch  3 : loss= 1.5148358345031738  acc= 0.3400000035762787
Batch  4 : loss= 1.4822825193405151  acc= 0.46000000834465027
Batch  5 : loss= 1.5379632711410522  acc= 0.3799999952316284
Batch  6 : loss= 1.5461478233337402  acc= 0.30000001192092896
Batch  7 : loss= 1.5734264850616455  acc= 0.2800000011920929
Test: Epoch  20 : loss= 1.5435542315244675  acc= 0.3375000022351742
Batch  0 : loss= 0.42277950048446655
Batch  1 : loss= 0.4087476134300232
Batch  2 : loss= 0.4207950234413147
Batch  3 : loss= 0.4159829616546631
Batch  4 : loss= 0.42711251974105835
Batch  5 : loss= 0.428827166557312
Batch  6 : loss= 0.42365527153015137
Batch  7 : loss= 0.42339521646499634
Batch  8 : loss= 0.42301249504089355
Batch  9 : loss= 0.42278003692626953
Batch  10 : loss= 0.4175724387168884
Batch  11 : loss= 0.43910133838653564
Batch  12 : loss= 0.43394553661346436
Batch  13 : loss= 0.430569052696228
Batch  14 : loss= 0.42063188552856445
Batch  15 : loss= 0.436471164226532
Batch  16 : loss= 0.40772777795791626
Batch  17 : loss= 0.3962405323982239
Batch  18 : loss= 0.40549492835998535
Batch  19 : loss= 0.3978639841079712
Train: Epoch  21 : loss= 0.4201353222131729
Batch  0 : loss= 1.449897050857544  acc= 0.3799999952316284
Batch  1 : loss= 1.3952827453613281  acc= 0.5799999833106995
Batch  2 : loss= 1.4192328453063965  acc= 0.5
Batch  3 : loss= 1.4198532104492188  acc= 0.5600000023841858
Train: Epoch  21 : loss= 1.4210664629936218  acc= 0.5049999952316284
Batch  0 : loss= 1.5058567523956299  acc= 0.4399999976158142
Batch  1 : loss= 1.560447335243225  acc= 0.30000001192092896
Batch  2 : loss= 1.5552014112472534  acc= 0.3400000035762787
Batch  3 : loss= 1.5551414489746094  acc= 0.25999999046325684
Batch  4 : loss= 1.4893344640731812  acc= 0.46000000834465027
Batch  5 : loss= 1.4880743026733398  acc= 0.41999998688697815
Batch  6 : loss= 1.5587716102600098  acc= 0.30000001192092896
Batch  7 : loss= 1.512560248374939  acc= 0.36000001430511475
Test: Epoch  21 : loss= 1.5281734466552734  acc= 0.36000000312924385
Batch  0 : loss= 0.4200248718261719
Batch  1 : loss= 0.4187198281288147
Batch  2 : loss= 0.42049407958984375
Batch  3 : loss= 0.4184643626213074
Batch  4 : loss= 0.4201318621635437
Batch  5 : loss= 0.4205644130706787
Batch  6 : loss= 0.4127635955810547
Batch  7 : loss= 0.43584340810775757
Batch  8 : loss= 0.43556028604507446
Batch  9 : loss= 0.43522995710372925
Batch  10 : loss= 0.42546600103378296
Batch  11 : loss= 0.4351962208747864
Batch  12 : loss= 0.4177953004837036
Batch  13 : loss= 0.4012985825538635
Batch  14 : loss= 0.41211384534835815
Batch  15 : loss= 0.4095427989959717
Batch  16 : loss= 0.40917086601257324
Batch  17 : loss= 0.4021347761154175
Batch  18 : loss= 0.4106086492538452
Batch  19 : loss= 0.4018275737762451
Train: Epoch  22 : loss= 0.41814756393432617
Batch  0 : loss= 1.5019923448562622  acc= 0.4000000059604645
Batch  1 : loss= 1.4291579723358154  acc= 0.5
Batch  2 : loss= 1.474058985710144  acc= 0.5
Batch  3 : loss= 1.4042387008666992  acc= 0.6399999856948853
Train: Epoch  22 : loss= 1.4523620009422302  acc= 0.5099999979138374
Batch  0 : loss= 1.5186212062835693  acc= 0.36000001430511475
Batch  1 : loss= 1.5270572900772095  acc= 0.41999998688697815
Batch  2 : loss= 1.5108880996704102  acc= 0.3799999952316284
Batch  3 : loss= 1.5307115316390991  acc= 0.30000001192092896
Batch  4 : loss= 1.4880361557006836  acc= 0.4000000059604645
Batch  5 : loss= 1.5301076173782349  acc= 0.36000001430511475
Batch  6 : loss= 1.515618085861206  acc= 0.46000000834465027
Batch  7 : loss= 1.5078293085098267  acc= 0.5
Test: Epoch  22 : loss= 1.51610866189003  acc= 0.39750000461935997
Batch  0 : loss= 0.4091167449951172
Batch  1 : loss= 0.4163967967033386
Batch  2 : loss= 0.40633463859558105
Batch  3 : loss= 0.43164247274398804
Batch  4 : loss= 0.4328939914703369
Batch  5 : loss= 0.4334462881088257
Batch  6 : loss= 0.4265736937522888
Batch  7 : loss= 0.4302101731300354
Batch  8 : loss= 0.4200226068496704
Batch  9 : loss= 0.4059433341026306
Batch  10 : loss= 0.4133012890815735
Batch  11 : loss= 0.4084080457687378
Batch  12 : loss= 0.4170563220977783
Batch  13 : loss= 0.40436410903930664
Batch  14 : loss= 0.41498494148254395
Batch  15 : loss= 0.41442859172821045
Batch  16 : loss= 0.40097296237945557
Batch  17 : loss= 0.4041900038719177
Batch  18 : loss= 0.38898783922195435
Batch  19 : loss= 0.40892934799194336
Train: Epoch  23 : loss= 0.4144102096557617
Batch  0 : loss= 1.4233050346374512  acc= 0.6200000047683716
Batch  1 : loss= 1.4319664239883423  acc= 0.5400000214576721
Batch  2 : loss= 1.404100775718689  acc= 0.6000000238418579
Batch  3 : loss= 1.4438897371292114  acc= 0.46000000834465027
Train: Epoch  23 : loss= 1.4258154928684235  acc= 0.555000014603138
Batch  0 : loss= 1.5153372287750244  acc= 0.3799999952316284
Batch  1 : loss= 1.5406312942504883  acc= 0.36000001430511475
Batch  2 : loss= 1.5114262104034424  acc= 0.4000000059604645
Batch  3 : loss= 1.535947322845459  acc= 0.3199999928474426
Batch  4 : loss= 1.5710692405700684  acc= 0.30000001192092896
Batch  5 : loss= 1.492226243019104  acc= 0.4000000059604645
Batch  6 : loss= 1.5056325197219849  acc= 0.4000000059604645
Batch  7 : loss= 1.4338222742080688  acc= 0.5199999809265137
Test: Epoch  23 : loss= 1.513261541724205  acc= 0.38500000163912773
Batch  0 : loss= 0.42957770824432373
Batch  1 : loss= 0.42708325386047363
Batch  2 : loss= 0.4247993230819702
Batch  3 : loss= 0.42378026247024536
Batch  4 : loss= 0.42387157678604126
Batch  5 : loss= 0.4066387414932251
Batch  6 : loss= 0.4163699746131897
Batch  7 : loss= 0.41036874055862427
Batch  8 : loss= 0.41968441009521484
Batch  9 : loss= 0.4108884334564209
Batch  10 : loss= 0.40963834524154663
Batch  11 : loss= 0.420082151889801
Batch  12 : loss= 0.41408056020736694
Batch  13 : loss= 0.4117111563682556
Batch  14 : loss= 0.39547282457351685
Batch  15 : loss= 0.4176861643791199
Batch  16 : loss= 0.42219579219818115
Batch  17 : loss= 0.41070419549942017
Batch  18 : loss= 0.4057871103286743
Batch  19 : loss= 0.4083070158958435
Train: Epoch  24 : loss= 0.41543638706207275
Batch  0 : loss= 1.3714468479156494  acc= 0.7200000286102295
Batch  1 : loss= 1.3930976390838623  acc= 0.6399999856948853
Batch  2 : loss= 1.373589038848877  acc= 0.6399999856948853
Batch  3 : loss= 1.4014716148376465  acc= 0.6200000047683716
Train: Epoch  24 : loss= 1.3849012851715088  acc= 0.6550000011920929
Batch  0 : loss= 1.5127736330032349  acc= 0.4000000059604645
Batch  1 : loss= 1.5142822265625  acc= 0.4399999976158142
Batch  2 : loss= 1.5317273139953613  acc= 0.3199999928474426
Batch  3 : loss= 1.5434269905090332  acc= 0.3799999952316284
Batch  4 : loss= 1.5343846082687378  acc= 0.3400000035762787
Batch  5 : loss= 1.5815125703811646  acc= 0.23999999463558197
Batch  6 : loss= 1.519652247428894  acc= 0.4399999976158142
Batch  7 : loss= 1.494911551475525  acc= 0.41999998688697815
Test: Epoch  24 : loss= 1.5290838927030563  acc= 0.37249999679625034
Batch  0 : loss= 0.42266619205474854
Batch  1 : loss= 0.3982729911804199
Batch  2 : loss= 0.4081289768218994
Batch  3 : loss= 0.40954577922821045
Batch  4 : loss= 0.41817402839660645
Batch  5 : loss= 0.4091331958770752
Batch  6 : loss= 0.41356873512268066
Batch  7 : loss= 0.4186335802078247
Batch  8 : loss= 0.41401296854019165
Batch  9 : loss= 0.40992939472198486
Batch  10 : loss= 0.402746319770813
Batch  11 : loss= 0.4103661775588989
Batch  12 : loss= 0.4340311884880066
Batch  13 : loss= 0.4177060127258301
Batch  14 : loss= 0.41194701194763184
Batch  15 : loss= 0.4181780219078064
Batch  16 : loss= 0.41129040718078613
Batch  17 : loss= 0.385894775390625
Batch  18 : loss= 0.39290738105773926
Batch  19 : loss= 0.38889503479003906
Train: Epoch  25 : loss= 0.4098014086484909
Batch  0 : loss= 1.3812075853347778  acc= 0.7400000095367432
Batch  1 : loss= 1.4254478216171265  acc= 0.5400000214576721
Batch  2 : loss= 1.4103187322616577  acc= 0.5
Batch  3 : loss= 1.4534655809402466  acc= 0.5799999833106995
Train: Epoch  25 : loss= 1.4176099300384521  acc= 0.5900000035762787
Batch  0 : loss= 1.517482042312622  acc= 0.36000001430511475
Batch  1 : loss= 1.4990249872207642  acc= 0.4000000059604645
Batch  2 : loss= 1.541286587715149  acc= 0.3400000035762787
Batch  3 : loss= 1.5347081422805786  acc= 0.3199999928474426
Batch  4 : loss= 1.5422536134719849  acc= 0.4000000059604645
Batch  5 : loss= 1.4667327404022217  acc= 0.4399999976158142
Batch  6 : loss= 1.5011199712753296  acc= 0.4399999976158142
Batch  7 : loss= 1.5278844833374023  acc= 0.41999998688697815
Test: Epoch  25 : loss= 1.5163115710020065  acc= 0.39000000059604645
Batch  0 : loss= 0.4125328063964844
Batch  1 : loss= 0.40592867136001587
Batch  2 : loss= 0.40952450037002563
Batch  3 : loss= 0.4101613163948059
Batch  4 : loss= 0.4087672829627991
Batch  5 : loss= 0.4087141156196594
Batch  6 : loss= 0.3988696336746216
Batch  7 : loss= 0.4168931841850281
Batch  8 : loss= 0.4303828477859497
Batch  9 : loss= 0.423622727394104
Batch  10 : loss= 0.4130713939666748
Batch  11 : loss= 0.4197855591773987
Batch  12 : loss= 0.4190177917480469
Batch  13 : loss= 0.40142935514450073
Batch  14 : loss= 0.39971280097961426
Batch  15 : loss= 0.39497649669647217
Batch  16 : loss= 0.4040645360946655
Batch  17 : loss= 0.3894311785697937
Batch  18 : loss= 0.4007689356803894
Batch  19 : loss= 0.3912634253501892
Train: Epoch  26 : loss= 0.40794592797756196
Batch  0 : loss= 1.4434902667999268  acc= 0.5600000023841858
Batch  1 : loss= 1.4293383359909058  acc= 0.5400000214576721
Batch  2 : loss= 1.4083117246627808  acc= 0.5600000023841858
Batch  3 : loss= 1.4615118503570557  acc= 0.46000000834465027
Train: Epoch  26 : loss= 1.4356630444526672  acc= 0.5300000086426735
Batch  0 : loss= 1.5090055465698242  acc= 0.41999998688697815
Batch  1 : loss= 1.514878511428833  acc= 0.36000001430511475
Batch  2 : loss= 1.51386296749115  acc= 0.4000000059604645
Batch  3 : loss= 1.500889778137207  acc= 0.4399999976158142
Batch  4 : loss= 1.5169099569320679  acc= 0.41999998688697815
Batch  5 : loss= 1.5047614574432373  acc= 0.4399999976158142
Batch  6 : loss= 1.578478217124939  acc= 0.23999999463558197
Batch  7 : loss= 1.5251771211624146  acc= 0.4000000059604645
Test: Epoch  26 : loss= 1.520495444536209  acc= 0.3899999987334013
Batch  0 : loss= 0.40540051460266113
Batch  1 : loss= 0.4054000973701477
Batch  2 : loss= 0.3968166708946228
Batch  3 : loss= 0.40701884031295776
Batch  4 : loss= 0.43348783254623413
Batch  5 : loss= 0.42304527759552
Batch  6 : loss= 0.41820406913757324
Batch  7 : loss= 0.4184262156486511
Batch  8 : loss= 0.4267549514770508
Batch  9 : loss= 0.4021131992340088
Batch  10 : loss= 0.410702645778656
Batch  11 : loss= 0.3989256024360657
Batch  12 : loss= 0.40723055601119995
Batch  13 : loss= 0.4015013575553894
Batch  14 : loss= 0.4088403582572937
Batch  15 : loss= 0.40651893615722656
Batch  16 : loss= 0.3964275121688843
Batch  17 : loss= 0.3971390724182129
Batch  18 : loss= 0.38501298427581787
Batch  19 : loss= 0.386191189289093
Train: Epoch  27 : loss= 0.40675789415836333
Batch  0 : loss= 1.3974002599716187  acc= 0.699999988079071
Batch  1 : loss= 1.4380314350128174  acc= 0.5799999833106995
Batch  2 : loss= 1.4036511182785034  acc= 0.6000000238418579
Batch  3 : loss= 1.4523835182189941  acc= 0.5799999833106995
Train: Epoch  27 : loss= 1.4228665828704834  acc= 0.614999994635582
Batch  0 : loss= 1.5110504627227783  acc= 0.3799999952316284
Batch  1 : loss= 1.5330514907836914  acc= 0.3199999928474426
Batch  2 : loss= 1.5862784385681152  acc= 0.25999999046325684
Batch  3 : loss= 1.4892269372940063  acc= 0.46000000834465027
Batch  4 : loss= 1.475002408027649  acc= 0.4000000059604645
Batch  5 : loss= 1.5350300073623657  acc= 0.36000001430511475
Batch  6 : loss= 1.4928134679794312  acc= 0.46000000834465027
Batch  7 : loss= 1.4825934171676636  acc= 0.5199999809265137
Test: Epoch  27 : loss= 1.5131308287382126  acc= 0.39499999955296516
Batch  0 : loss= 0.42233431339263916
Batch  1 : loss= 0.42591971158981323
Batch  2 : loss= 0.4147729277610779
Batch  3 : loss= 0.4133256673812866
Batch  4 : loss= 0.42682570219039917
Batch  5 : loss= 0.40383416414260864
Batch  6 : loss= 0.4021819829940796
Batch  7 : loss= 0.39880073070526123
Batch  8 : loss= 0.4081302285194397
Batch  9 : loss= 0.40525388717651367
Batch  10 : loss= 0.4133068323135376
Batch  11 : loss= 0.4093055725097656
Batch  12 : loss= 0.4047579765319824
Batch  13 : loss= 0.4051930904388428
Batch  14 : loss= 0.39121395349502563
Batch  15 : loss= 0.39490437507629395
Batch  16 : loss= 0.4156237244606018
Batch  17 : loss= 0.40862828493118286
Batch  18 : loss= 0.40496909618377686
Batch  19 : loss= 0.395966112613678
Train: Epoch  28 : loss= 0.4082624167203903
Batch  0 : loss= 1.4625499248504639  acc= 0.5
Batch  1 : loss= 1.366988182067871  acc= 0.6800000071525574
Batch  2 : loss= 1.3954777717590332  acc= 0.6399999856948853
Batch  3 : loss= 1.436920166015625  acc= 0.5199999809265137
Train: Epoch  28 : loss= 1.4154840111732483  acc= 0.5849999934434891
Batch  0 : loss= 1.529659390449524  acc= 0.3799999952316284
Batch  1 : loss= 1.5305601358413696  acc= 0.36000001430511475
Batch  2 : loss= 1.5064902305603027  acc= 0.41999998688697815
Batch  3 : loss= 1.5003254413604736  acc= 0.4399999976158142
Batch  4 : loss= 1.556870698928833  acc= 0.30000001192092896
Batch  5 : loss= 1.547532320022583  acc= 0.36000001430511475
Batch  6 : loss= 1.5007460117340088  acc= 0.4000000059604645
Batch  7 : loss= 1.472557783126831  acc= 0.47999998927116394
Test: Epoch  28 : loss= 1.5180927515029907  acc= 0.39250000193715096
Batch  0 : loss= 0.41736406087875366
Batch  1 : loss= 0.4016764163970947
Batch  2 : loss= 0.3955225348472595
Batch  3 : loss= 0.4000488519668579
Batch  4 : loss= 0.40253138542175293
Batch  5 : loss= 0.40548181533813477
Batch  6 : loss= 0.40982258319854736
Batch  7 : loss= 0.4118309020996094
Batch  8 : loss= 0.4043760299682617
Batch  9 : loss= 0.4062686562538147
Batch  10 : loss= 0.39619094133377075
Batch  11 : loss= 0.39980846643447876
Batch  12 : loss= 0.4167517423629761
Batch  13 : loss= 0.4129331707954407
Batch  14 : loss= 0.41008979082107544
Batch  15 : loss= 0.40520769357681274
Batch  16 : loss= 0.41565221548080444
Batch  17 : loss= 0.3859449028968811
Batch  18 : loss= 0.3835170269012451
Batch  19 : loss= 0.3853936195373535
Train: Epoch  29 : loss= 0.40332064032554626
Batch  0 : loss= 1.4200750589370728  acc= 0.5799999833106995
Batch  1 : loss= 1.4421718120574951  acc= 0.5
Batch  2 : loss= 1.3642327785491943  acc= 0.6800000071525574
Batch  3 : loss= 1.4182194471359253  acc= 0.5600000023841858
Train: Epoch  29 : loss= 1.4111747741699219  acc= 0.5799999982118607
Batch  0 : loss= 1.489710807800293  acc= 0.4399999976158142
Batch  1 : loss= 1.5459562540054321  acc= 0.36000001430511475
Batch  2 : loss= 1.554111361503601  acc= 0.30000001192092896
Batch  3 : loss= 1.5081892013549805  acc= 0.3799999952316284
Batch  4 : loss= 1.4805134534835815  acc= 0.46000000834465027
Batch  5 : loss= 1.5163899660110474  acc= 0.36000001430511475
Batch  6 : loss= 1.5098453760147095  acc= 0.4000000059604645
Batch  7 : loss= 1.5032196044921875  acc= 0.4399999976158142
Test: Epoch  29 : loss= 1.513492003083229  acc= 0.39250000566244125
Batch  0 : loss= 0.39660972356796265
Batch  1 : loss= 0.39868438243865967
Batch  2 : loss= 0.4011029005050659
Batch  3 : loss= 0.40948933362960815
Batch  4 : loss= 0.4026069641113281
Batch  5 : loss= 0.4018310308456421
Batch  6 : loss= 0.4043475389480591
Batch  7 : loss= 0.3912985324859619
Batch  8 : loss= 0.4169674515724182
Batch  9 : loss= 0.40894073247909546
Batch  10 : loss= 0.4129045009613037
Batch  11 : loss= 0.4079713225364685
Batch  12 : loss= 0.4183831810951233
Batch  13 : loss= 0.3955247402191162
Batch  14 : loss= 0.38814759254455566
Batch  15 : loss= 0.39178627729415894
Batch  16 : loss= 0.3859691619873047
Batch  17 : loss= 0.3887631297111511
Batch  18 : loss= 0.3824421763420105
Batch  19 : loss= 0.39933985471725464
Train: Epoch  30 : loss= 0.40015552639961244
Batch  0 : loss= 1.4300273656845093  acc= 0.6000000238418579
Batch  1 : loss= 1.4012795686721802  acc= 0.6600000262260437
Batch  2 : loss= 1.3940696716308594  acc= 0.6200000047683716
Batch  3 : loss= 1.4041775465011597  acc= 0.5799999833106995
Train: Epoch  30 : loss= 1.4073885381221771  acc= 0.6150000095367432
Batch  0 : loss= 1.5135236978530884  acc= 0.3400000035762787
Batch  1 : loss= 1.53779935836792  acc= 0.3199999928474426
Batch  2 : loss= 1.5272160768508911  acc= 0.4000000059604645
Batch  3 : loss= 1.512790322303772  acc= 0.4000000059604645
Batch  4 : loss= 1.534103512763977  acc= 0.4399999976158142
Batch  5 : loss= 1.5017284154891968  acc= 0.3400000035762787
Batch  6 : loss= 1.5045099258422852  acc= 0.4399999976158142
Batch  7 : loss= 1.5177990198135376  acc= 0.36000001430511475
Test: Epoch  30 : loss= 1.5186837911605835  acc= 0.380000002682209
Batch  0 : loss= 0.39706504344940186
Batch  1 : loss= 0.3944336175918579
Batch  2 : loss= 0.40313005447387695
Batch  3 : loss= 0.3862781524658203
Batch  4 : loss= 0.4161461591720581
Batch  5 : loss= 0.41155701875686646
Batch  6 : loss= 0.4174423813819885
Batch  7 : loss= 0.4040994644165039
Batch  8 : loss= 0.4171961545944214
Batch  9 : loss= 0.40098506212234497
Batch  10 : loss= 0.39063018560409546
Batch  11 : loss= 0.3975118398666382
Batch  12 : loss= 0.3863733410835266
Batch  13 : loss= 0.39820241928100586
Batch  14 : loss= 0.38865339756011963
Batch  15 : loss= 0.40281957387924194
Batch  16 : loss= 0.3955731987953186
Batch  17 : loss= 0.3851528763771057
Batch  18 : loss= 0.3885110020637512
Batch  19 : loss= 0.3689413070678711
Train: Epoch  31 : loss= 0.39753511250019075
Batch  0 : loss= 1.4127389192581177  acc= 0.6200000047683716
Batch  1 : loss= 1.4007970094680786  acc= 0.6399999856948853
Batch  2 : loss= 1.4152064323425293  acc= 0.6200000047683716
Batch  3 : loss= 1.3968348503112793  acc= 0.6399999856948853
Train: Epoch  31 : loss= 1.4063943028450012  acc= 0.6299999952316284
Batch  0 : loss= 1.5337090492248535  acc= 0.36000001430511475
Batch  1 : loss= 1.4781478643417358  acc= 0.5199999809265137
Batch  2 : loss= 1.5457650423049927  acc= 0.3199999928474426
Batch  3 : loss= 1.5354520082473755  acc= 0.3199999928474426
Batch  4 : loss= 1.5071481466293335  acc= 0.36000001430511475
Batch  5 : loss= 1.5203280448913574  acc= 0.4000000059604645
Batch  6 : loss= 1.5208184719085693  acc= 0.3799999952316284
Batch  7 : loss= 1.465626835823059  acc= 0.47999998927116394
Test: Epoch  31 : loss= 1.5133744329214096  acc= 0.39249999821186066
Batch  0 : loss= 0.4073309898376465
Batch  1 : loss= 0.4088747501373291
Batch  2 : loss= 0.4126032590866089
Batch  3 : loss= 0.4047924280166626
Batch  4 : loss= 0.40904974937438965
Batch  5 : loss= 0.40458041429519653
Batch  6 : loss= 0.39167582988739014
Batch  7 : loss= 0.4001479744911194
Batch  8 : loss= 0.38402390480041504
Batch  9 : loss= 0.3995521664619446
Batch  10 : loss= 0.394126296043396
Batch  11 : loss= 0.39851903915405273
Batch  12 : loss= 0.40081167221069336
Batch  13 : loss= 0.39751136302948
Batch  14 : loss= 0.3960219621658325
Batch  15 : loss= 0.378423273563385
Batch  16 : loss= 0.39456093311309814
Batch  17 : loss= 0.4004690647125244
Batch  18 : loss= 0.39627605676651
Batch  19 : loss= 0.3871370553970337
Train: Epoch  32 : loss= 0.39832440912723543
Batch  0 : loss= 1.4445881843566895  acc= 0.5400000214576721
Batch  1 : loss= 1.4239966869354248  acc= 0.5799999833106995
Batch  2 : loss= 1.4401764869689941  acc= 0.5600000023841858
Batch  3 : loss= 1.4741474390029907  acc= 0.5
Train: Epoch  32 : loss= 1.4457271993160248  acc= 0.5450000017881393
Batch  0 : loss= 1.514225721359253  acc= 0.3199999928474426
Batch  1 : loss= 1.5380276441574097  acc= 0.36000001430511475
Batch  2 : loss= 1.4823994636535645  acc= 0.5
Batch  3 : loss= 1.4777045249938965  acc= 0.47999998927116394
Batch  4 : loss= 1.5076873302459717  acc= 0.3799999952316284
Batch  5 : loss= 1.4998997449874878  acc= 0.41999998688697815
Batch  6 : loss= 1.5067367553710938  acc= 0.4000000059604645
Batch  7 : loss= 1.5051047801971436  acc= 0.4000000059604645
Test: Epoch  32 : loss= 1.5039732456207275  acc= 0.4074999988079071
Batch  0 : loss= 0.4051804542541504
Batch  1 : loss= 0.40431779623031616
Batch  2 : loss= 0.38625025749206543
Batch  3 : loss= 0.3945389986038208
Batch  4 : loss= 0.38557928800582886
Batch  5 : loss= 0.3967876434326172
Batch  6 : loss= 0.3946334719657898
Batch  7 : loss= 0.4014018177986145
Batch  8 : loss= 0.4006807804107666
Batch  9 : loss= 0.3967745304107666
Batch  10 : loss= 0.3957850933074951
Batch  11 : loss= 0.389095664024353
Batch  12 : loss= 0.3930450677871704
Batch  13 : loss= 0.4190082550048828
Batch  14 : loss= 0.4058316946029663
Batch  15 : loss= 0.3951495885848999
Batch  16 : loss= 0.4007754921913147
Batch  17 : loss= 0.3909838795661926
Batch  18 : loss= 0.37343651056289673
Batch  19 : loss= 0.3818337321281433
Train: Epoch  33 : loss= 0.39555450081825255
Batch  0 : loss= 1.4382439851760864  acc= 0.5799999833106995
Batch  1 : loss= 1.3898037672042847  acc= 0.6200000047683716
Batch  2 : loss= 1.3748522996902466  acc= 0.6000000238418579
Batch  3 : loss= 1.4624983072280884  acc= 0.47999998927116394
Train: Epoch  33 : loss= 1.4163495898246765  acc= 0.5700000002980232
Batch  0 : loss= 1.5279484987258911  acc= 0.4000000059604645
Batch  1 : loss= 1.5442336797714233  acc= 0.3799999952316284
Batch  2 : loss= 1.5178489685058594  acc= 0.4000000059604645
Batch  3 : loss= 1.5310449600219727  acc= 0.3799999952316284
Batch  4 : loss= 1.4980677366256714  acc= 0.4399999976158142
Batch  5 : loss= 1.5306147336959839  acc= 0.3400000035762787
Batch  6 : loss= 1.505429983139038  acc= 0.41999998688697815
Batch  7 : loss= 1.5050877332687378  acc= 0.4000000059604645
Test: Epoch  33 : loss= 1.5200345367193222  acc= 0.39499999955296516
Batch  0 : loss= 0.38021284341812134
Batch  1 : loss= 0.39800262451171875
Batch  2 : loss= 0.3916603922843933
Batch  3 : loss= 0.40092140436172485
Batch  4 : loss= 0.39998143911361694
Batch  5 : loss= 0.39230436086654663
Batch  6 : loss= 0.39512401819229126
Batch  7 : loss= 0.3889251947402954
Batch  8 : loss= 0.39945709705352783
Batch  9 : loss= 0.4195460081100464
Batch  10 : loss= 0.40794700384140015
Batch  11 : loss= 0.398384690284729
Batch  12 : loss= 0.40619421005249023
Batch  13 : loss= 0.4019097685813904
Batch  14 : loss= 0.3882451057434082
Batch  15 : loss= 0.38625770807266235
Batch  16 : loss= 0.3744620680809021
Batch  17 : loss= 0.38747817277908325
Batch  18 : loss= 0.37637829780578613
Batch  19 : loss= 0.389093816280365
Train: Epoch  34 : loss= 0.394124311208725
Batch  0 : loss= 1.4373115301132202  acc= 0.5400000214576721
Batch  1 : loss= 1.371924877166748  acc= 0.6399999856948853
Batch  2 : loss= 1.4374421834945679  acc= 0.5600000023841858
Batch  3 : loss= 1.3976988792419434  acc= 0.6399999856948853
Train: Epoch  34 : loss= 1.4110943675041199  acc= 0.5949999988079071
Batch  0 : loss= 1.4903279542922974  acc= 0.5600000023841858
Batch  1 : loss= 1.5060266256332397  acc= 0.46000000834465027
Batch  2 : loss= 1.5137094259262085  acc= 0.3799999952316284
Batch  3 : loss= 1.5440198183059692  acc= 0.3400000035762787
Batch  4 : loss= 1.5126070976257324  acc= 0.36000001430511475
Batch  5 : loss= 1.4932899475097656  acc= 0.3799999952316284
Batch  6 : loss= 1.5475484132766724  acc= 0.3400000035762787
Batch  7 : loss= 1.512660026550293  acc= 0.41999998688697815
Test: Epoch  34 : loss= 1.5150236636400223  acc= 0.4050000011920929
Batch  0 : loss= 0.39187300205230713
Batch  1 : loss= 0.39097774028778076
Batch  2 : loss= 0.3921934962272644
Batch  3 : loss= 0.3853647708892822
Batch  4 : loss= 0.39302247762680054
Batch  5 : loss= 0.4219864010810852
Batch  6 : loss= 0.4054613709449768
Batch  7 : loss= 0.4019291400909424
Batch  8 : loss= 0.40362852811813354
Batch  9 : loss= 0.4073569178581238
Batch  10 : loss= 0.38564127683639526
Batch  11 : loss= 0.3909302353858948
Batch  12 : loss= 0.3774619698524475
Batch  13 : loss= 0.38850855827331543
Batch  14 : loss= 0.38751816749572754
Batch  15 : loss= 0.39172685146331787
Batch  16 : loss= 0.3864154815673828
Batch  17 : loss= 0.38024789094924927
Batch  18 : loss= 0.38138753175735474
Batch  19 : loss= 0.3735184073448181
Train: Epoch  35 : loss= 0.39185751080513
Batch  0 : loss= 1.3580714464187622  acc= 0.6600000262260437
Batch  1 : loss= 1.4727822542190552  acc= 0.5
Batch  2 : loss= 1.4248151779174805  acc= 0.5799999833106995
Batch  3 : loss= 1.3860818147659302  acc= 0.6399999856948853
Train: Epoch  35 : loss= 1.410437673330307  acc= 0.5949999988079071
Batch  0 : loss= 1.4490472078323364  acc= 0.5400000214576721
Batch  1 : loss= 1.551945447921753  acc= 0.30000001192092896
Batch  2 : loss= 1.5063937902450562  acc= 0.41999998688697815
Batch  3 : loss= 1.508516788482666  acc= 0.41999998688697815
Batch  4 : loss= 1.5404672622680664  acc= 0.36000001430511475
Batch  5 : loss= 1.5576214790344238  acc= 0.30000001192092896
Batch  6 : loss= 1.5224462747573853  acc= 0.3799999952316284
Batch  7 : loss= 1.451675295829773  acc= 0.5600000023841858
Test: Epoch  35 : loss= 1.5110141932964325  acc= 0.4100000038743019
Batch  0 : loss= 0.3819330930709839
Batch  1 : loss= 0.4094090461730957
Batch  2 : loss= 0.4091278314590454
Batch  3 : loss= 0.4015280604362488
Batch  4 : loss= 0.3979714512825012
Batch  5 : loss= 0.40766823291778564
Batch  6 : loss= 0.3887336850166321
Batch  7 : loss= 0.3834129571914673
Batch  8 : loss= 0.3802703619003296
Batch  9 : loss= 0.39143216609954834
Batch  10 : loss= 0.3883002996444702
Batch  11 : loss= 0.39730119705200195
Batch  12 : loss= 0.3913915157318115
Batch  13 : loss= 0.3888695240020752
Batch  14 : loss= 0.38594377040863037
Batch  15 : loss= 0.3843201994895935
Batch  16 : loss= 0.37714189291000366
Batch  17 : loss= 0.40246206521987915
Batch  18 : loss= 0.3908611536026001
Batch  19 : loss= 0.3917943835258484
Train: Epoch  36 : loss= 0.3924936443567276
Batch  0 : loss= 1.4238468408584595  acc= 0.5
Batch  1 : loss= 1.4247015714645386  acc= 0.5600000023841858
Batch  2 : loss= 1.452635645866394  acc= 0.5400000214576721
Batch  3 : loss= 1.4160561561584473  acc= 0.5799999833106995
Train: Epoch  36 : loss= 1.4293100535869598  acc= 0.5450000017881393
Batch  0 : loss= 1.5400241613388062  acc= 0.3400000035762787
Batch  1 : loss= 1.5158450603485107  acc= 0.4000000059604645
Batch  2 : loss= 1.515901803970337  acc= 0.47999998927116394
Batch  3 : loss= 1.5627357959747314  acc= 0.2800000011920929
Batch  4 : loss= 1.5294346809387207  acc= 0.4000000059604645
Batch  5 : loss= 1.5418111085891724  acc= 0.3400000035762787
Batch  6 : loss= 1.5194836854934692  acc= 0.36000001430511475
Batch  7 : loss= 1.5260376930236816  acc= 0.3799999952316284
Test: Epoch  36 : loss= 1.5314092487096786  acc= 0.3725000023841858
Batch  0 : loss= 0.3923335671424866
Batch  1 : loss= 0.3998386859893799
Batch  2 : loss= 0.3859711289405823
Batch  3 : loss= 0.37571293115615845
Batch  4 : loss= 0.3834400177001953
Batch  5 : loss= 0.38605839014053345
Batch  6 : loss= 0.38832807540893555
Batch  7 : loss= 0.3945842981338501
Batch  8 : loss= 0.3909796476364136
Batch  9 : loss= 0.38292092084884644
Batch  10 : loss= 0.38553839921951294
Batch  11 : loss= 0.3867639899253845
Batch  12 : loss= 0.38409191370010376
Batch  13 : loss= 0.40727418661117554
Batch  14 : loss= 0.39749306440353394
Batch  15 : loss= 0.3961619734764099
Batch  16 : loss= 0.38356560468673706
Batch  17 : loss= 0.3949996829032898
Batch  18 : loss= 0.3689337372779846
Batch  19 : loss= 0.3655058741569519
Train: Epoch  37 : loss= 0.38752480447292326
Batch  0 : loss= 1.3715150356292725  acc= 0.6000000238418579
Batch  1 : loss= 1.410589337348938  acc= 0.5400000214576721
Batch  2 : loss= 1.3795275688171387  acc= 0.6200000047683716
Batch  3 : loss= 1.3510801792144775  acc= 0.6600000262260437
Train: Epoch  37 : loss= 1.3781780302524567  acc= 0.6050000190734863
Batch  0 : loss= 1.451389193534851  acc= 0.5
Batch  1 : loss= 1.5202900171279907  acc= 0.3400000035762787
Batch  2 : loss= 1.4960732460021973  acc= 0.4000000059604645
Batch  3 : loss= 1.4804054498672485  acc= 0.46000000834465027
Batch  4 : loss= 1.4946603775024414  acc= 0.46000000834465027
Batch  5 : loss= 1.475562572479248  acc= 0.47999998927116394
Batch  6 : loss= 1.499543309211731  acc= 0.3400000035762787
Batch  7 : loss= 1.4944672584533691  acc= 0.41999998688697815
Test: Epoch  37 : loss= 1.4890489280223846  acc= 0.42500000074505806
Batch  0 : loss= 0.37796342372894287
Batch  1 : loss= 0.38174861669540405
Batch  2 : loss= 0.38661378622055054
Batch  3 : loss= 0.3858426809310913
Batch  4 : loss= 0.3902636766433716
Batch  5 : loss= 0.38601553440093994
Batch  6 : loss= 0.38423335552215576
Batch  7 : loss= 0.3893902897834778
Batch  8 : loss= 0.38051384687423706
Batch  9 : loss= 0.4076908826828003
Batch  10 : loss= 0.3979511857032776
Batch  11 : loss= 0.4025254249572754
Batch  12 : loss= 0.3886624574661255
Batch  13 : loss= 0.3996236324310303
Batch  14 : loss= 0.3822207450866699
Batch  15 : loss= 0.36770886182785034
Batch  16 : loss= 0.3752734065055847
Batch  17 : loss= 0.3696233034133911
Batch  18 : loss= 0.3774402141571045
Batch  19 : loss= 0.3698825240135193
Train: Epoch  38 : loss= 0.38505939245223997
Batch  0 : loss= 1.4563319683074951  acc= 0.6000000238418579
Batch  1 : loss= 1.3666236400604248  acc= 0.6600000262260437
Batch  2 : loss= 1.3122066259384155  acc= 0.7400000095367432
Batch  3 : loss= 1.4097778797149658  acc= 0.5400000214576721
Train: Epoch  38 : loss= 1.3862350285053253  acc= 0.6350000202655792
Batch  0 : loss= 1.501305341720581  acc= 0.47999998927116394
Batch  1 : loss= 1.5480287075042725  acc= 0.3799999952316284
Batch  2 : loss= 1.5365357398986816  acc= 0.3799999952316284
Batch  3 : loss= 1.4764580726623535  acc= 0.5
Batch  4 : loss= 1.5267541408538818  acc= 0.4399999976158142
Batch  5 : loss= 1.4592710733413696  acc= 0.5
Batch  6 : loss= 1.4466699361801147  acc= 0.5
Batch  7 : loss= 1.5288951396942139  acc= 0.4000000059604645
Test: Epoch  38 : loss= 1.5029897689819336  acc= 0.44749999791383743
Batch  0 : loss= 0.3907889723777771
Batch  1 : loss= 0.38431334495544434
Batch  2 : loss= 0.38187074661254883
Batch  3 : loss= 0.39072561264038086
Batch  4 : loss= 0.37968671321868896
Batch  5 : loss= 0.4055144786834717
Batch  6 : loss= 0.40253716707229614
Batch  7 : loss= 0.4027581214904785
Batch  8 : loss= 0.38994288444519043
Batch  9 : loss= 0.40167236328125
Batch  10 : loss= 0.3922255039215088
Batch  11 : loss= 0.3752622604370117
Batch  12 : loss= 0.38078272342681885
Batch  13 : loss= 0.37439417839050293
Batch  14 : loss= 0.3869211673736572
Batch  15 : loss= 0.37660902738571167
Batch  16 : loss= 0.38264942169189453
Batch  17 : loss= 0.37976551055908203
Batch  18 : loss= 0.37024205923080444
Batch  19 : loss= 0.37706202268600464
Train: Epoch  39 : loss= 0.3862862139940262
Batch  0 : loss= 1.39801824092865  acc= 0.5400000214576721
Batch  1 : loss= 1.3618316650390625  acc= 0.6399999856948853
Batch  2 : loss= 1.3842015266418457  acc= 0.6200000047683716
Batch  3 : loss= 1.4059776067733765  acc= 0.6000000238418579
Train: Epoch  39 : loss= 1.3875072598457336  acc= 0.6000000089406967
Batch  0 : loss= 1.493094801902771  acc= 0.4399999976158142
Batch  1 : loss= 1.5422667264938354  acc= 0.3199999928474426
Batch  2 : loss= 1.5240318775177002  acc= 0.36000001430511475
Batch  3 : loss= 1.4112566709518433  acc= 0.5600000023841858
Batch  4 : loss= 1.5758014917373657  acc= 0.25999999046325684
Batch  5 : loss= 1.4912375211715698  acc= 0.4399999976158142
Batch  6 : loss= 1.4553632736206055  acc= 0.5199999809265137
Batch  7 : loss= 1.4851089715957642  acc= 0.46000000834465027
Test: Epoch  39 : loss= 1.4972701668739319  acc= 0.41999999806284904
Batch  0 : loss= 0.3707754611968994
Batch  1 : loss= 0.39527320861816406
Batch  2 : loss= 0.40291792154312134
Batch  3 : loss= 0.4004763960838318
Batch  4 : loss= 0.38903796672821045
Batch  5 : loss= 0.3962894082069397
Batch  6 : loss= 0.39336252212524414
Batch  7 : loss= 0.3750120997428894
Batch  8 : loss= 0.3809524178504944
Batch  9 : loss= 0.37399202585220337
Batch  10 : loss= 0.3856898546218872
Batch  11 : loss= 0.3802144527435303
Batch  12 : loss= 0.37977051734924316
Batch  13 : loss= 0.38761281967163086
Batch  14 : loss= 0.38166290521621704
Batch  15 : loss= 0.3828822374343872
Batch  16 : loss= 0.3614068627357483
Batch  17 : loss= 0.38024234771728516
Batch  18 : loss= 0.39453601837158203
Batch  19 : loss= 0.3834764361381531
Train: Epoch  40 : loss= 0.38477919399738314
Batch  0 : loss= 1.4140032529830933  acc= 0.6600000262260437
Batch  1 : loss= 1.431557059288025  acc= 0.5600000023841858
Batch  2 : loss= 1.3828762769699097  acc= 0.6600000262260437
Batch  3 : loss= 1.429824709892273  acc= 0.6000000238418579
Train: Epoch  40 : loss= 1.4145653247833252  acc= 0.6200000196695328
Batch  0 : loss= 1.5274266004562378  acc= 0.2800000011920929
Batch  1 : loss= 1.4962401390075684  acc= 0.4000000059604645
Batch  2 : loss= 1.5224900245666504  acc= 0.4000000059604645
Batch  3 : loss= 1.5069482326507568  acc= 0.41999998688697815
Batch  4 : loss= 1.5210915803909302  acc= 0.4399999976158142
Batch  5 : loss= 1.5127140283584595  acc= 0.41999998688697815
Batch  6 : loss= 1.523891806602478  acc= 0.41999998688697815
Batch  7 : loss= 1.490586757659912  acc= 0.41999998688697815
Test: Epoch  40 : loss= 1.5126736462116241  acc= 0.3999999947845936
Batch  0 : loss= 0.38193678855895996
Batch  1 : loss= 0.3958808183670044
Batch  2 : loss= 0.39498382806777954
Batch  3 : loss= 0.3724241256713867
Batch  4 : loss= 0.3782070279121399
Batch  5 : loss= 0.3741380572319031
Batch  6 : loss= 0.3856657147407532
Batch  7 : loss= 0.37817418575286865
